{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinalPipeline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMeKqJVrcTJwTqXyDXHwnDN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WqR0GnSYfG10"},"source":["In the final pipeline we will consider the model with attention based beam search adn bidirectional GRU (without augmentation) because this model gave the most satisfactory results"]},{"cell_type":"markdown","metadata":{"id":"sR9koMIgOLh0"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"jkfuvALlJZkX","executionInfo":{"status":"ok","timestamp":1606393304587,"user_tz":-330,"elapsed":960,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import os\n","import sklearn\n","import tqdm\n","from tqdm import tqdm \n","import nltk\n","import warnings\n","warnings.filterwarnings(\"ignore\") \n","import cv2\n","from sklearn.model_selection import train_test_split\n","import PIL\n","from PIL import Image\n","import time\n","\n","import tensorflow as tf\n","import keras\n","from keras.layers import Input,Dense,Conv2D,concatenate,Dropout,LSTM\n","from keras import Model\n","from tensorflow.keras import activations\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import nltk.translate.bleu_score as bleu\n","from keras.models import load_model\n"],"execution_count":336,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9nOr_SeTMGEP","executionInfo":{"status":"ok","timestamp":1606393304979,"user_tz":-330,"elapsed":1330,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}},"outputId":"130b0064-1954-413f-b3bb-ca13b4a835e3"},"source":["from google.colab import drive \n","drive.mount('/content/drive') \n"],"execution_count":337,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rRjpI8t_MF5S","executionInfo":{"status":"ok","timestamp":1606393304980,"user_tz":-330,"elapsed":1309,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["os.chdir(\"/content/drive/My Drive/Self Case Study 2\")   "],"execution_count":338,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksdtQiSfMFje","executionInfo":{"status":"ok","timestamp":1606393309690,"user_tz":-330,"elapsed":5996,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["test=pd.read_csv(\"test\") "],"execution_count":340,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EorCWFl1NS1P"},"source":["### Loading the feature extraction model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiEm8zyyMFuJ","executionInfo":{"status":"ok","timestamp":1606393309689,"user_tz":-330,"elapsed":6004,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}},"outputId":"f3b91c5f-c7d0-4ea6-93bf-032a279c4b74"},"source":["final_chexnet_model=load_model(\"chexnet.h5\")"],"execution_count":339,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oQvIRVu0NY0B"},"source":["### Function for extracting image features using the loaded chexnet model"]},{"cell_type":"code","metadata":{"id":"GVIjoS2jMFZu","executionInfo":{"status":"ok","timestamp":1606393309691,"user_tz":-330,"elapsed":5990,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["def feature_extraction(image_1,image_2):\n","\n","  #normalize the values of the image\n","  image_1=image_1/255\n","  image_2=image_2/255\n","\n","    #resize all image into (224,224)\n","  image_1 = cv2.resize(image_1,(224,224))\n","  image_2 = cv2.resize(image_2,(224,224))\n","    \n","  image_1= np.expand_dims(image_1, axis=0)\n","  image_2= np.expand_dims(image_2, axis=0)\n","    \n","    #now we have read two image per patient. this is goven to the chexnet model for feature extraction\n","    \n","  image_1_out=final_chexnet_model(image_1)\n","  image_2_out=final_chexnet_model(image_2)\n","  #conactenate along the width\n","  conc=np.concatenate((image_1_out,image_2_out),axis=2)\n","  #reshape into(no.of images passed, length*breadth, depth)\n","  image_feature=tf.reshape(conc, (conc.shape[0], -1, conc.shape[-1]))\n","  \n","\n","  \n","  return image_feature\n"],"execution_count":341,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XZFCYlHyNpcR"},"source":["### Take a random test data point,and obtain the image features using the loaded feature extraction model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XohOc4EPM1LE","executionInfo":{"status":"ok","timestamp":1606393310849,"user_tz":-330,"elapsed":7137,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}},"outputId":"291747c6-484d-43ee-aa39-4d4de737fb58"},"source":["import random \n","\n","a=random.sample(range(764),1)[0]\n","\n","print(a)\n","img1=test.iloc[a][\"image1\"] \n","img2=test.iloc[a][\"image2\"]\n","image_1 = Image.open(img1)\n","  \n","image_1= np.asarray(image_1.convert(\"RGB\"))\n","  \n","  \n","image_2 = Image.open(img2)\n","image_2 = np.asarray(image_2.convert(\"RGB\"))\n","  \n","image_feature=feature_extraction(image_1,image_2)"],"execution_count":342,"outputs":[{"output_type":"stream","text":["577\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7e8I0-CnNycK"},"source":["### Defining the encoder-decoder model"]},{"cell_type":"code","metadata":{"id":"1xwNTZaPahZ5","executionInfo":{"status":"ok","timestamp":1606393310856,"user_tz":-330,"elapsed":7135,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["embedding_matrix=np.load('emb.npy')"],"execution_count":343,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMy-nsTqM1AP","executionInfo":{"status":"ok","timestamp":1606393310857,"user_tz":-330,"elapsed":7124,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["#encoder model\n","'''\n","here the input will be image features with size (96,1024). We can consider this tensor as the encoder output.\n","But here we add another dense layer that will reduce the depth of this feature from 1024 to a low value\n","'''\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self,units):\n","    super().__init__()\n","    self.units=units\n","    \n","  \n","  def build(self,input_shape):\n","    self.dense1=Dense(self.units,activation=\"relu\",name=\"encoder_dense\")\n","    self.maxpool=tf.keras.layers.Dropout(0.5)\n","\n","  def call(self,input_):\n","    enc_out=self.maxpool(input_)\n","    enc_out=self.dense1(enc_out) \n","    \n","    return enc_out\n","    \n","  def initialize_states(self,batch_size):\n","      '''\n","      Given a batch size it will return intial hidden state\n","      If batch size is 32- Hidden state shape is [32,units]\n","      '''\n","      forward_h=tf.zeros((batch_size,self.units))\n","      back_h=tf.zeros((batch_size,self.units))\n","      return forward_h,back_h\n"],"execution_count":344,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5L3GpIaY4Rr","executionInfo":{"status":"ok","timestamp":1606393310858,"user_tz":-330,"elapsed":7117,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["'''\n","this is the attention class. \n","Here the input to the decoder and the gru hidden state at the pevious time step are given, and the context vector is calculated\n","\n","This context vector is calculated uisng the attention weights. This context vector is then passed to the decoder model\n","\n","Here conact function is used for calaculating the attention weights\n","\n","'''\n","\n","class Attention(tf.keras.layers.Layer):\n","\n","  def __init__(self,att_units):\n","\n","    super().__init__()\n","    \n","    self.att_units=att_units\n","\n","  def build(self,input_shape):\n","    self.wa=tf.keras.layers.Dense(self.att_units)\n","    self.wb=tf.keras.layers.Dense(self.att_units)\n","    self.v=tf.keras.layers.Dense(1)\n","  \n","    \n","  def call(self,decoder_hidden_state,encoder_output):\n","   \n","    x=tf.expand_dims(decoder_hidden_state,1)\n","    \n","    # print(x.shape)\n","    # print(encoder_output.shape)\n","      \n","    alpha_dash=self.v(tf.nn.tanh(self.wa(encoder_output)+self.wb(x)))\n","    \n","    alphas=tf.nn.softmax(alpha_dash,1)\n","\n","    # print(\"en\",encoder_output.shape)\n","    # print(\"al\",alphas.shape)\n","    \n","    context_vector=tf.matmul(encoder_output,alphas,transpose_a=True)[:,:,0]\n","    # context_vector = alphas*encoder_output\n","    # print(\"c\",context_vector.shape)\n","\n","\n","    return (context_vector,alphas)\n","        \n","        \n","    \n","    "],"execution_count":345,"outputs":[]},{"cell_type":"code","metadata":{"id":"yW6GgVZ9ZkgK","executionInfo":{"status":"ok","timestamp":1606393310858,"user_tz":-330,"elapsed":7108,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["'''\n","This class will perform the decoder task.\n","The main decoder will call this onestep decoder at every time step. This one step decoder in turn class the atention model and return the ouptput \n","at time step t.\n","This output is passed through the final softmax layer with output size =vocab size, and pass this result to the main decoder model\n","\n","'''\n","\n","class One_Step_Decoder(tf.keras.Model):\n","  def __init__(self,vocab_size, embedding_dim, input_length, dec_units ,att_units):\n","\n","      # Initialize decoder embedding layer, LSTM and any other objects needed\n","    super().__init__()\n","    \n","    self.att_units=att_units\n","    self.vocab_size=vocab_size\n","    self.embedding_dim=embedding_dim\n","    self.input_length=input_length\n","    \n","    self.dec_units=dec_units\n","    self.attention=Attention(self.att_units)\n","  #def build(self,inp_shape):\n","    self.embedding=tf.keras.layers.Embedding(self.vocab_size,output_dim=self.embedding_dim,\n","                                             input_length=self.input_length,mask_zero=True,trainable=False,weights=[embedding_matrix])\n","\n","    self.gru= tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.dec_units,return_sequences=True, return_state=True))\n","    self.dense=tf.keras.layers.Dense(self.vocab_size,name=\"decoder_final_dense\") \n","    self.dense_2=tf.keras.layers.Dense(self.embedding_dim,name=\"decoder_dense2\")\n","\n","  def call(self,input_to_decoder, encoder_output, for_h,bac_h):\n","    \n","    embed=self.embedding(input_to_decoder)\n","    state_h=tf.keras.layers.Add()([for_h,bac_h])\n","    \n","\n","    context_vector,alpha=self.attention(state_h,encoder_output)\n","    context_vector=self.dense_2(context_vector)\n","    \n","    result=tf.concat([tf.expand_dims(context_vector, axis=1),embed],axis=-1)\n","    \n","   \n","    output,forward_h,back_h=self.gru(result,initial_state=[for_h,bac_h])\n","    out=tf.reshape(output,(-1,output.shape[-1]))\n","\n","    out=tf.keras.layers.Dropout(0.5)(out)\n","    \n","    dense_op=self.dense(out)\n","    \n","    return dense_op,forward_h,back_h,alpha"],"execution_count":346,"outputs":[]},{"cell_type":"code","metadata":{"id":"EdqobNOVZkRa","executionInfo":{"status":"ok","timestamp":1606393310859,"user_tz":-330,"elapsed":7102,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["'''\n","For every input sentence, each output word is generated using one step decoder. Each output word is stored using the final decoder model and the\n","final output sentence is returned\n","\n","'''\n","\n","class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, output_length, dec_units,att_units):\n","      super().__init__()\n","      #Intialize necessary variables and create an object from the class onestepdecoder\n","      self.onestep=One_Step_Decoder(vocab_size, embedding_dim, output_length, dec_units,att_units)\n","\n","\n","        \n","    def call(self, input_to_decoder,encoder_output,state_1,state_2):\n","\n","        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n","        #Create a tensor array as shown in the reference notebook\n","        \n","        #Iterate till the length of the decoder input\n","            # Call onestepdecoder for each token in decoder_input\n","            # Store the output in tensorarray\n","        # Return the tensor array\n","        \n","        all_outputs=tf.TensorArray(tf.float32,input_to_decoder.shape[1],name=\"output_array\")\n","        for step in range(input_to_decoder.shape[1]):\n","          output,state_1,state_2,alpha=self.onestep(input_to_decoder[:,step:step+1],encoder_output,state_1,state_2)\n","\n","          all_outputs=all_outputs.write(step,output)\n","        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n","        \n","        return all_outputs\n","    "],"execution_count":347,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUbyY_aYZuFd","executionInfo":{"status":"ok","timestamp":1606393310859,"user_tz":-330,"elapsed":7093,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["class encoder_decoder(tf.keras.Model):\n","  def __init__(self,enc_units,embedding_dim,vocab_size,output_length,dec_units,att_units,batch_size):\n","        super().__init__()\n","\n","        \n","        self.batch_size=batch_size\n","        self.encoder =Encoder(enc_units)\n","        self.decoder=Decoder(vocab_size,embedding_dim,output_length,dec_units,att_units)\n","        \n","  \n","    #Coompute the image features using feature extraction model and pass it to the encoder\n","    # This will give encoder ouput\n","   # Pass the decoder sequence,encoder_output,initial states to Decoder\n","    # return the decoder output\n","\n","  \n","  def call(self, data):\n","        features,report  = data[0], data[1]\n","        \n","        encoder_output= self.encoder(features)\n","        state_h,back_h=self.encoder.initialize_states(self.batch_size)\n","        \n","        output= self.decoder(report, encoder_output,state_h,back_h)\n","      \n","        return output\n","\n","    \n","      "],"execution_count":348,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxD7hZ_8Zt33","executionInfo":{"status":"ok","timestamp":1606393310860,"user_tz":-330,"elapsed":7087,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["enc_units=64\n","embedding_dim=300\n","dec_units=64\n","att_units=64\n","max_len=80\n","vocab_size=2017\n","bs=5\n","model  = encoder_decoder(enc_units,embedding_dim,vocab_size,max_len,dec_units,att_units,bs)\n"],"execution_count":349,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsSAbXn8bL6i"},"source":["### Loading the pretrained weights to the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gy0XyyYWZ0fU","executionInfo":{"status":"ok","timestamp":1606393310862,"user_tz":-330,"elapsed":7062,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}},"outputId":"91680814-c1a4-41b1-fb4e-f6c1be122c85"},"source":["model.load_weights(\"model2wts/bidir_fit_15_b\") "],"execution_count":352,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7efda9f2e4a8>"]},"metadata":{"tags":[]},"execution_count":352}]},{"cell_type":"markdown","metadata":{"id":"9fuokLL-N5W8"},"source":["### Prediction of the report using beam search"]},{"cell_type":"code","metadata":{"id":"E5zvivbdbGle","executionInfo":{"status":"ok","timestamp":1606393310863,"user_tz":-330,"elapsed":7050,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["#Loading the pretrained tokenizer\n","import pickle\n","with open('tokenizer.pickle', 'rb') as handle:\n","    token= pickle.load(handle)"],"execution_count":353,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vz4whzrpLxPT","executionInfo":{"status":"ok","timestamp":1606393310864,"user_tz":-330,"elapsed":7045,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}}},"source":["def take_second(elem):\n","    return elem[1]\n","\n","def beam_search(image_features, beam_index):\n","\n","    hidden_state =  tf.zeros((1, enc_units))\n","\n","    hidden_state_2 =  tf.zeros((1, enc_units))\n","    encoder_out = model.layers[0](image_features)\n","\n","    start_token = [token.word_index[\"<sos>\"]]\n","    dec_word = [[start_token, 0.0]]\n","    while len(dec_word[0][0]) < max_len:\n","        temp = []\n","        for word in dec_word:\n","            \n","            predict, hidden_state,hidden_state_2,alpha = model.layers[1].onestep(tf.expand_dims([word[0][-1]],1), encoder_out, hidden_state,hidden_state_2)\n","           \n","           \n","            word_predict = np.argsort(predict[0])[-beam_index:]\n","            for i in word_predict:\n","\n","                next_word, probab = word[0][:], word[1]\n","                next_word.append(i)\n","                probab += predict[0][i] \n","                temp.append([next_word, probab.numpy()])\n","        dec_word = temp\n","        # Sorting according to the probabilities scores\n","        \n","        \n","        dec_word = sorted(dec_word, key=take_second)\n","       \n","        # Getting the top words\n","        dec_word = dec_word[-beam_index:] \n","        \n","     \n","    final = dec_word[-1]\n","    \n","    report =final[0]\n","    score = final[1]\n","    temp = []\n","    \n","    for word in report:\n","      if word!=0:\n","        if word != token.word_index['<eos>']:\n","            temp.append(token.index_word[word])\n","        else:\n","            break \n","\n","    rep = ' '.join(e for e in temp)        \n","    \n","    return rep, score"],"execution_count":354,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z61kSpqZL0x7","executionInfo":{"status":"ok","timestamp":1606392756356,"user_tz":-330,"elapsed":14568,"user":{"displayName":"VINITHA V N","photoUrl":"","userId":"14038784479246566841"}},"outputId":"cf55d07e-ad69-4587-8a16-5bfae45d2bac"},"source":["\n","\n","result,score=beam_search(image_feature,3) \n","\n","actual=test[\"report\"][a]\n","  \n","print(\"ACTUAL REPORT: \",actual)\n","print(\"GENERATED REPORT: \",result)\n","\n","print(\"BLEU SCORE IS: \",bleu.sentence_bleu(actual,result))   \n"],"execution_count":275,"outputs":[{"output_type":"stream","text":["ACTUAL REPORT:  there are several small calcified granulomas. the lungs are otherwise clear. focal airspace consolidation. suspicious pulmonary mass nodule identified. there pleural effusion pneumothorax. heart size and mediastinal contour are within normal limits. there are diffuse degenerative changes the spine. \n","GENERATED REPORT:  <sos> the mediastinum within normal size and pleural effusion pneumothorax.\n","BLEU SCORE IS:  0.7186082239261684\n"],"name":"stdout"}]}]}